{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5",
      "metadata": {
        "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5"
      },
      "source": [
        "# RLSS2023 - DQN Tutorial: Deep Q-Network (DQN)\n",
        "\n",
        "## Part II: DQN Update and Training Loop\n",
        "\n",
        "Website: https://rlsummerschool.com/\n",
        "\n",
        "Github repository: https://github.com/araffin/rlss23-dqn-tutorial\n",
        "\n",
        "Gymnasium documentation: https://gymnasium.farama.org/\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this notebook, you will finish the implementation of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm (started in part I) by implementing the training loop and the DQN gradient update."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyter_black"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esAVpBFyL_0j",
        "outputId": "c279c5e1-c489-4bae-f26c-3c36f1f51db3"
      },
      "id": "esAVpBFyL_0j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyter_black\n",
            "  Downloading jupyter_black-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Collecting black>=21 (from jupyter_black)\n",
            "  Downloading black-24.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=7 in /usr/local/lib/python3.10/dist-packages (from jupyter_black) (7.34.0)\n",
            "Collecting tokenize-rt>=4 (from jupyter_black)\n",
            "  Downloading tokenize_rt-5.2.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black>=21->jupyter_black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black>=21->jupyter_black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black>=21->jupyter_black) (24.0)\n",
            "Collecting pathspec>=0.9.0 (from black>=21->jupyter_black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black>=21->jupyter_black) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black>=21->jupyter_black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black>=21->jupyter_black) (4.10.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7->jupyter_black)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7->jupyter_black) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7->jupyter_black) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7->jupyter_black) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7->jupyter_black) (0.2.13)\n",
            "Installing collected packages: tokenize-rt, pathspec, mypy-extensions, jedi, black, jupyter_black\n",
            "Successfully installed black-24.3.0 jedi-0.19.1 jupyter_black-0.3.4 mypy-extensions-1.0.0 pathspec-0.12.1 tokenize-rt-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe1a9aa-5735-4614-9a76-031656397899",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7fe1a9aa-5735-4614-9a76-031656397899",
        "outputId": "103adb50-0ecc-4308-88f7-ea41ce154b79"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
              "                (function() {\n",
              "                    if (window.IPython === undefined) {\n",
              "                        return\n",
              "                    }\n",
              "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
              "                        \"jupyter_black in a non-lab notebook with \" +\n",
              "                        \"`is_lab=True`. Please double check, and if \" +\n",
              "                        \"loading with `%load_ext` please review the README!\"\n",
              "                    console.log(msg)\n",
              "                    alert(msg)\n",
              "                })()\n",
              "                </script>\n",
              "                "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# for autoformatting\n",
        "%load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8188798b-daf5-43a7-91ec-a7a922bc2034",
      "metadata": {
        "id": "8188798b-daf5-43a7-91ec-a7a922bc2034"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b55494c-fff2-4459-87e1-e7399afd56d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b55494c-fff2-4459-87e1-e7399afd56d5",
        "outputId": "010cdcab-6dbb-44c0-99f6-75d8ea203399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/araffin/rlss23-dqn-tutorial/\n",
            "  Cloning https://github.com/araffin/rlss23-dqn-tutorial/ to /tmp/pip-req-build-2ey_l0xv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/araffin/rlss23-dqn-tutorial/ /tmp/pip-req-build-2ey_l0xv\n",
            "  Resolved https://github.com/araffin/rlss23-dqn-tutorial/ to commit 3070ad0d11f0ece870fb0f6978dc75911edeaca1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dqn_tutorial==0.1.dev52+g3070ad0) (1.25.2)\n",
            "Collecting gymnasium[classic-control]<1.0,>=0.28.1 (from dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from dqn_tutorial==0.1.dev52+g3070ad0) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from dqn_tutorial==0.1.dev52+g3070ad0) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]<1.0,>=0.28.1->dqn_tutorial==0.1.dev52+g3070ad0) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]<1.0,>=0.28.1->dqn_tutorial==0.1.dev52+g3070ad0) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control]<1.0,>=0.28.1->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]<1.0,>=0.28.1->dqn_tutorial==0.1.dev52+g3070ad0) (2.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m778.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dqn_tutorial==0.1.dev52+g3070ad0) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dqn_tutorial==0.1.dev52+g3070ad0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dqn_tutorial==0.1.dev52+g3070ad0) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->dqn_tutorial==0.1.dev52+g3070ad0) (1.3.0)\n",
            "Building wheels for collected packages: dqn_tutorial\n",
            "  Building wheel for dqn_tutorial (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dqn_tutorial: filename=dqn_tutorial-0.1.dev52+g3070ad0-py3-none-any.whl size=19451 sha256=fbd44887bba6797f6383a9e0d2a24ddbef7da6f02d87076523d8ec2cc6b76531\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mjl120pn/wheels/eb/01/e5/e10c90ef93e4de9e05cf38717b3ab8a9bdc71982eb2eecf9a3\n",
            "Successfully built dqn_tutorial\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dqn_tutorial\n",
            "Successfully installed dqn_tutorial-0.1.dev52+g3070ad0 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/araffin/rlss23-dqn-tutorial/ --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600ca128-eaab-4cda-a378-8b122bb214ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600ca128-eaab-4cda-a378-8b122bb214ca",
        "outputId": "c852acf9-6aec-402a-9e25-cc3476306427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install ffmpeg  # For visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30fc32dc-45f6-4303-ab92-4d0f4cc34c67",
      "metadata": {
        "id": "30fc32dc-45f6-4303-ab92-4d0f4cc34c67"
      },
      "source": [
        "### Imports (from Part I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34722639-6c01-4c5b-ab65-c8d5cb2adaad",
      "metadata": {
        "id": "34722639-6c01-4c5b-ab65-c8d5cb2adaad"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# We implemented those components in part I\n",
        "from dqn_tutorial.dqn import ReplayBuffer, epsilon_greedy_action_selection, collect_one_step, linear_schedule, QNetwork\n",
        "from dqn_tutorial.dqn.evaluation import evaluate_policy\n",
        "from dqn_tutorial.notebook_utils import show_videos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82559d16-3fc6-4d47-a373-f94adcab7102",
      "metadata": {
        "id": "82559d16-3fc6-4d47-a373-f94adcab7102"
      },
      "source": [
        "## DQN Update rule (no target network)\n",
        "<div>\n",
        "    <img src=\"attachment:692b5681-cf53-4b6b-9684-4ca0f393b069.png\" width=\"1000\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41c2092-ffa8-4369-a5ac-e9b81fd2c12f",
      "metadata": {
        "id": "c41c2092-ffa8-4369-a5ac-e9b81fd2c12f"
      },
      "source": [
        "### Exercise (20 minutes): write DQN update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3fb635-fa44-4a52-88be-1c17d5a639c7",
      "metadata": {
        "id": "0d3fb635-fa44-4a52-88be-1c17d5a639c7"
      },
      "outputs": [],
      "source": [
        "def dqn_update_no_target(\n",
        "    q_net: QNetwork,\n",
        "    optimizer: th.optim.Optimizer,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    batch_size: int,\n",
        "    gamma: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform one gradient step on the Q-network\n",
        "    using the data from the replay buffer.\n",
        "    Note: this is the same as dqn_update in dqn.py, but without the target network.\n",
        "\n",
        "    :param q_net: The Q-network to update\n",
        "    :param optimizer: The optimizer to use\n",
        "    :param replay_buffer: The replay buffer containing the transitions\n",
        "    :param batch_size: The minibatch size, how many transitions to sample\n",
        "    :param gamma: The discount factor\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Sample the replay buffer and convert them to PyTorch tensors\n",
        "    # using `.to_torch()` method\n",
        "    replay_data = replay_buffer.sample(batch_size).to_torch()\n",
        "\n",
        "    # We should not compute gradient with respect to the target\n",
        "    with th.no_grad():\n",
        "        # Compute the Q-values for the next observations (batch_size, n_actions)\n",
        "        next_q_values = q_net(replay_data.next_observations)\n",
        "        # Follow greedy policy: use the one with the highest value\n",
        "        # shape: (batch_size,)\n",
        "        next_q_values, _ = next_q_values.max(dim=1)\n",
        "        # If the episode is terminated, set the target to the reward\n",
        "        # (same as FQI, you can use `th.logical_not` to mask the next q values)\n",
        "        should_bootstrap = th.logical_not(replay_data.terminateds)\n",
        "        # 1-step TD target\n",
        "        td_target = replay_data.rewards + gamma * next_q_values * should_bootstrap\n",
        "\n",
        "    # Get current Q-values estimates for the replay_data (batch_size, n_actions)\n",
        "    q_values = q_net(replay_data.observations)\n",
        "    # Select the Q-values corresponding to the actions that were selected\n",
        "    # during data collection,\n",
        "    # you should use `th.gather()`\n",
        "    current_q_values = th.gather(q_values, dim=1, index=replay_data.actions)\n",
        "    # Reshape from (batch_size, 1) to (batch_size,) to avoid broadcast error\n",
        "    current_q_values = current_q_values.squeeze(dim=1)\n",
        "\n",
        "    # Check for any shape/broadcast error\n",
        "    # Current q-values must have the same shape as the TD target\n",
        "    assert current_q_values.shape == (batch_size,), f\"{current_q_values.shape} != {(batch_size,)}\"\n",
        "    assert current_q_values.shape == td_target.shape, f\"{current_q_values.shape} != {td_target.shape}\"\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) loss\n",
        "    # Optionally, one can use a Huber loss instead of the MSE loss\n",
        "    loss = ((current_q_values - td_target) ** 2).mean()\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameters of the q-network\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c7b067-cd2d-4dbb-a6b5-dd7cfb515137",
      "metadata": {
        "id": "d4c7b067-cd2d-4dbb-a6b5-dd7cfb515137"
      },
      "source": [
        "Let's test the implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab535b42-e643-422a-8087-51311b4cdb54",
      "metadata": {
        "id": "ab535b42-e643-422a-8087-51311b4cdb54"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "q_net = QNetwork(env.observation_space, env.action_space)\n",
        "optimizer = th.optim.Adam(q_net.parameters(), lr=0.001)\n",
        "replay_buffer = ReplayBuffer(2000, env.observation_space, env.action_space)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "# Let's collect some data following an epsilon-greedy policy\n",
        "for _ in range(1000):\n",
        "    obs = collect_one_step(env, q_net, replay_buffer, obs, exploration_rate=0.1)\n",
        "\n",
        "# Try to do some gradient steps:\n",
        "for _ in range(10):\n",
        "    dqn_update_no_target(q_net, optimizer, replay_buffer, batch_size=32, gamma=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515755c2-f802-4662-8337-918d1505fc9a",
      "metadata": {
        "id": "515755c2-f802-4662-8337-918d1505fc9a"
      },
      "source": [
        "### Exercise (10 minutes): write the training loop\n",
        "\n",
        "Let's put everything together and implement the training loop that alternates between data collection and updating the Q-Network.\n",
        "At first we will not use any target network.\n",
        "\n",
        "<div>\n",
        "    <img src=\"attachment:a9db6a84-2a29-469b-b414-73890c37cdfb.png\" width=\"600\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790401fe-f7cc-4338-a6d4-b7996d275dd2",
      "metadata": {
        "id": "790401fe-f7cc-4338-a6d4-b7996d275dd2"
      },
      "outputs": [],
      "source": [
        "def run_dqn_no_target(\n",
        "    env_id: str = \"CartPole-v1\",\n",
        "    replay_buffer_size: int = 50_000,\n",
        "    # Exploration schedule\n",
        "    # (for the epsilon-greedy data collection)\n",
        "    exploration_initial_eps: float = 1.0,\n",
        "    exploration_final_eps: float = 0.01,\n",
        "    n_timesteps: int = 20_000,\n",
        "    update_interval: int = 2,\n",
        "    learning_rate: float = 3e-4,\n",
        "    batch_size: int = 64,\n",
        "    gamma: float = 0.99,\n",
        "    n_eval_episodes: int = 10,\n",
        "    evaluation_interval: int = 1000,\n",
        "    eval_exploration_rate: float = 0.0,\n",
        "    seed: int = 2023,\n",
        "    # device: Union[th.device, str] = \"cpu\",\n",
        "    eval_render_mode: Optional[str] = None,  # \"human\", \"rgb_array\", None\n",
        ") -> QNetwork:\n",
        "    \"\"\"\n",
        "    Run Deep Q-Learning (DQN) on a given environment.\n",
        "    (without target network)\n",
        "\n",
        "    :param env_id: Name of the environment\n",
        "    :param replay_buffer_size: Max capacity of the replay buffer\n",
        "    :param exploration_initial_eps: The initial exploration rate\n",
        "    :param exploration_final_eps: The final exploration rate\n",
        "    :param n_timesteps: Number of timesteps in total\n",
        "    :param update_interval: How often to update the Q-network\n",
        "        (every update_interval steps)\n",
        "    :param learning_rate: The learning rate to use for the optimizer\n",
        "    :param batch_size: The minibatch size\n",
        "    :param gamma: The discount factor\n",
        "    :param n_eval_episodes: The number of episodes to evaluate the policy on\n",
        "    :param evaluation_interval: How often to evaluate the policy\n",
        "    :param eval_exploration_rate: The exploration rate to use during evaluation\n",
        "    :param seed: Random seed for the pseudo random generator\n",
        "    :param eval_render_mode: The render mode to use for evaluation\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    # Seed Numpy as PyTorch pseudo random generators\n",
        "    # Seed Numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    th.manual_seed(seed)\n",
        "\n",
        "    # Create the environment\n",
        "    env = gym.make(env_id)\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    assert isinstance(env.action_space, spaces.Discrete)\n",
        "    env.action_space.seed(seed)\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(env_id, render_mode=eval_render_mode)\n",
        "    eval_env.reset(seed=seed)\n",
        "    eval_env.action_space.seed(seed)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO:\n",
        "    # 1. Instantiate the Q-Network and the optimizer\n",
        "    # 2. Instantiate the replay buffer\n",
        "    # 3. Compute the current exploration rate (epsilon)\n",
        "    # 4. Collect new transition by stepping in the env following\n",
        "    # an epsilon-greedy strategy\n",
        "    # 5. Update the Q-Network using gradient descent\n",
        "\n",
        "    # Create the q-network\n",
        "    q_net = QNetwork(env.observation_space, env.action_space)\n",
        "    # Create the optimizer (PyTorch `th.optim.Adam` will be helpful here)\n",
        "    optimizer = th.optim.Adam(q_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create the Replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_size, env.observation_space, env.action_space)\n",
        "    # Reset the env\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "\n",
        "    for current_step in range(1, n_timesteps + 1):\n",
        "        # Compute the current exploration rate\n",
        "        # according to the exploration schedule (update the value of epsilon)\n",
        "        # you should use `linear_schedule()`\n",
        "        exploration_rate = linear_schedule(\n",
        "            exploration_initial_eps,\n",
        "            exploration_final_eps,\n",
        "            current_step,\n",
        "            n_timesteps,\n",
        "        )\n",
        "        # Do one step in the environment following an epsilon-greedy policy\n",
        "        # and store the transition in the replay buffer\n",
        "        # you can re-use `collect_one_step()`\n",
        "        obs = collect_one_step(env, q_net, replay_buffer, obs, exploration_rate=exploration_rate)\n",
        "\n",
        "        # Update the Q-Network every `update_interval` steps\n",
        "        if (current_step % update_interval) == 0:\n",
        "            # Do one gradient step (using `dqn_update_no_target()`)\n",
        "            dqn_update_no_target(q_net, optimizer, replay_buffer, batch_size, gamma=gamma)\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "        if (current_step % evaluation_interval) == 0:\n",
        "            print()\n",
        "            print(f\"Evaluation at step {current_step}:\")\n",
        "            # Evaluate the current greedy policy (deterministic policy)\n",
        "            evaluate_policy(eval_env, q_net, n_eval_episodes, eval_exploration_rate=eval_exploration_rate)\n",
        "    return q_net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c35cd4-9d23-4083-9384-482fdaf1e9e6",
      "metadata": {
        "id": "79c35cd4-9d23-4083-9384-482fdaf1e9e6"
      },
      "source": [
        "## Train a DQN agent on CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d24455f-5cb6-4b2c-8b48-2f395bbce8e1",
      "metadata": {
        "id": "3d24455f-5cb6-4b2c-8b48-2f395bbce8e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# create log folder\n",
        "os.makedirs(\"../logs/\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3967c2a6-469f-4766-9911-fab855f945d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3967c2a6-469f-4766-9911-fab855f945d8",
        "outputId": "41eda46d-9c16-49ba-c989-6f999a08a620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation at step 1000:\n",
            "Mean episode reward: 9.20 +/- 0.75\n",
            "\n",
            "Evaluation at step 2000:\n",
            "Mean episode reward: 9.50 +/- 0.67\n",
            "\n",
            "Evaluation at step 3000:\n",
            "Mean episode reward: 50.40 +/- 10.47\n",
            "\n",
            "Evaluation at step 4000:\n",
            "Mean episode reward: 88.80 +/- 41.77\n",
            "\n",
            "Evaluation at step 5000:\n",
            "Mean episode reward: 109.20 +/- 31.57\n",
            "\n",
            "Evaluation at step 6000:\n",
            "Mean episode reward: 110.50 +/- 26.14\n",
            "\n",
            "Evaluation at step 7000:\n",
            "Mean episode reward: 221.90 +/- 145.50\n",
            "\n",
            "Evaluation at step 8000:\n",
            "Mean episode reward: 403.30 +/- 133.11\n",
            "\n",
            "Evaluation at step 9000:\n",
            "Mean episode reward: 316.00 +/- 82.25\n",
            "\n",
            "Evaluation at step 10000:\n",
            "Mean episode reward: 164.50 +/- 58.70\n",
            "\n",
            "Evaluation at step 11000:\n",
            "Mean episode reward: 208.40 +/- 59.28\n",
            "\n",
            "Evaluation at step 12000:\n",
            "Mean episode reward: 327.30 +/- 119.10\n",
            "\n",
            "Evaluation at step 13000:\n",
            "Mean episode reward: 304.40 +/- 125.29\n",
            "\n",
            "Evaluation at step 14000:\n",
            "Mean episode reward: 370.80 +/- 108.71\n",
            "\n",
            "Evaluation at step 15000:\n",
            "Mean episode reward: 303.90 +/- 112.74\n",
            "\n",
            "Evaluation at step 16000:\n",
            "Mean episode reward: 424.20 +/- 103.95\n",
            "\n",
            "Evaluation at step 17000:\n",
            "Mean episode reward: 301.00 +/- 81.90\n",
            "\n",
            "Evaluation at step 18000:\n",
            "Mean episode reward: 357.70 +/- 75.31\n",
            "\n",
            "Evaluation at step 19000:\n",
            "Mean episode reward: 391.50 +/- 107.65\n",
            "\n",
            "Evaluation at step 20000:\n",
            "Mean episode reward: 364.70 +/- 92.44\n"
          ]
        }
      ],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "q_net = run_dqn_no_target(env_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acb6e99-8328-4c6c-be81-32845d4d3dda",
      "metadata": {
        "id": "5acb6e99-8328-4c6c-be81-32845d4d3dda"
      },
      "source": [
        "### Record and show video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b988f9-05b0-46e3-a143-d43162d5cd7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "e4b988f9-05b0-46e3-a143-d43162d5cd7a",
        "outputId": "243196f8-d734-4f8c-af6e-b1fa5efa184a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video to /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_no_target_CartPole-v1.mp4\n",
            "Moviepy - Building video /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_no_target_CartPole-v1.mp4.\n",
            "Moviepy - Writing video /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_no_target_CartPole-v1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_no_target_CartPole-v1.mp4\n",
            "Mean episode reward: 466.67 +/- 26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.0\n",
        "video_name = f\"DQN_no_target_{env_id}\"\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(\"../logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b269a237-a0db-40a4-9621-449538c410b7",
      "metadata": {
        "id": "b269a237-a0db-40a4-9621-449538c410b7"
      },
      "source": [
        "## [Bonus] DQN Target Network\n",
        "\n",
        "\n",
        "<div>\n",
        "    <img src=\"attachment:c176b66c-2fc7-4e1e-a351-87ef832bdfac.png\" width=\"1000\"/>\n",
        "</div>\n",
        "\n",
        "The only things that is changing is when predicting the next q value.\n",
        "\n",
        "In DQN without target, the online network with weights **$\\theta$** is used:\n",
        "\n",
        "$y = r_t + \\gamma \\cdot \\max_{a \\in A}(\\hat{Q}_{\\pi}(s_{t+1}, a; \\theta))$\n",
        "\n",
        "\n",
        "whereas with DQN with target network, the target q-network (a delayed copy of the q-network) with weights **$\\theta^\\prime$** is used instead:\n",
        "\n",
        "$y = r_t + \\gamma \\cdot \\max_{a \\in A}(\\hat{Q}_{\\pi}(s_{t+1}, a; \\theta^\\prime))$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "781e7e78-b156-43f0-aa4d-f7c842d3d837",
      "metadata": {
        "id": "781e7e78-b156-43f0-aa4d-f7c842d3d837"
      },
      "source": [
        "### Exercise (5 minutes): write the DQN update with target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f17152-59cf-495a-bde7-73b6f219e779",
      "metadata": {
        "id": "27f17152-59cf-495a-bde7-73b6f219e779"
      },
      "outputs": [],
      "source": [
        "def dqn_update(\n",
        "    q_net: QNetwork,\n",
        "    q_target_net: QNetwork,\n",
        "    optimizer: th.optim.Optimizer,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    batch_size: int,\n",
        "    gamma: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform one gradient step on the Q-network\n",
        "    using the data from the replay buffer.\n",
        "\n",
        "    :param q_net: The Q-network to update\n",
        "    :param q_target_net: The target Q-network, to compute the td-target.\n",
        "    :param optimizer: The optimizer to use\n",
        "    :param replay_buffer: The replay buffer containing the transitions\n",
        "    :param batch_size: The minibatch size, how many transitions to sample\n",
        "    :param gamma: The discount factor\n",
        "    \"\"\"\n",
        "\n",
        "    # Sample the replay buffer and convert them to PyTorch tensors\n",
        "    replay_data = replay_buffer.sample(batch_size).to_torch()\n",
        "\n",
        "    with th.no_grad():\n",
        "        ### YOUR CODE HERE\n",
        "        # TODO: use the target q-network instead of the online q-network\n",
        "        # to compute the next values\n",
        "\n",
        "        # Compute the Q-values for the next observations (batch_size, n_actions)\n",
        "        # using the target network\n",
        "        next_q_values = q_target_net(replay_data.next_observations)\n",
        "        # Follow greedy policy: use the one with the highest value\n",
        "        # (batch_size,)\n",
        "        next_q_values, _ = next_q_values.max(dim=1)\n",
        "        # If the episode is terminated, set the target to the reward\n",
        "        should_bootstrap = th.logical_not(replay_data.terminateds)\n",
        "        # 1-step TD target\n",
        "        td_target = replay_data.rewards + gamma * next_q_values * should_bootstrap\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    # Get current Q-values estimates for the replay_data (batch_size, n_actions)\n",
        "    q_values = q_net(replay_data.observations)\n",
        "    # Select the Q-values corresponding to the actions that were selected\n",
        "    # during data collection\n",
        "    current_q_values = th.gather(q_values, dim=1, index=replay_data.actions)\n",
        "    # Reshape from (batch_size, 1) to (batch_size,) to avoid broadcast error\n",
        "    current_q_values = current_q_values.squeeze(dim=1)\n",
        "\n",
        "    # Check for any shape/broadcast error\n",
        "    # Current q-values must have the same shape as the TD target\n",
        "    assert current_q_values.shape == (batch_size,), f\"{current_q_values.shape} != {(batch_size,)}\"\n",
        "    assert current_q_values.shape == td_target.shape, f\"{current_q_values.shape} != {td_target.shape}\"\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) loss\n",
        "    # Optionally, one can use a Huber loss instead of the MSE loss\n",
        "    loss = ((current_q_values - td_target) ** 2).mean()\n",
        "    # Huber loss\n",
        "    # loss = th.nn.functional.smooth_l1_loss(current_q_values, td_target)\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameters of the q-network\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c74b0e-1923-4c6e-9a58-f83a97435f0d",
      "metadata": {
        "id": "85c74b0e-1923-4c6e-9a58-f83a97435f0d"
      },
      "source": [
        "### Updated training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484",
      "metadata": {
        "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484"
      },
      "outputs": [],
      "source": [
        "def run_dqn(\n",
        "    env_id: str = \"CartPole-v1\",\n",
        "    replay_buffer_size: int = 50_000,\n",
        "    # How often do we copy the parameters from the Q-network to the target network\n",
        "    target_network_update_interval: int = 1000,\n",
        "    # Warmup phase\n",
        "    learning_starts: int = 100,\n",
        "    # Exploration schedule\n",
        "    # (for the epsilon-greedy data collection)\n",
        "    exploration_initial_eps: float = 1.0,\n",
        "    exploration_final_eps: float = 0.01,\n",
        "    exploration_fraction: float = 0.1,\n",
        "    n_timesteps: int = 20_000,\n",
        "    update_interval: int = 2,\n",
        "    learning_rate: float = 3e-4,\n",
        "    batch_size: int = 64,\n",
        "    gamma: float = 0.99,\n",
        "    n_hidden_units: int = 64,\n",
        "    n_eval_episodes: int = 10,\n",
        "    evaluation_interval: int = 1000,\n",
        "    eval_exploration_rate: float = 0.0,\n",
        "    seed: int = 2023,\n",
        "    # device: Union[th.device, str] = \"cpu\",\n",
        "    eval_render_mode: Optional[str] = None,  # \"human\", \"rgb_array\", None\n",
        ") -> QNetwork:\n",
        "    \"\"\"\n",
        "    Run Deep Q-Learning (DQN) on a given environment.\n",
        "    (with a target network)\n",
        "\n",
        "    :param env_id: Name of the environment\n",
        "    :param replay_buffer_size: Max capacity of the replay buffer\n",
        "    :param target_network_update_interval: How often do we copy the parameters\n",
        "         to the target network\n",
        "    :param learning_starts: Warmup phase to fill the replay buffer\n",
        "        before starting the optimization.\n",
        "    :param exploration_initial_eps: The initial exploration rate\n",
        "    :param exploration_final_eps: The final exploration rate\n",
        "    :param exploration_fraction: The fraction of the number of steps\n",
        "        during which the exploration rate is annealed from\n",
        "        initial_eps to final_eps.\n",
        "        After this many steps, the exploration rate remains constant.\n",
        "    :param n_timesteps: Number of timesteps in total\n",
        "    :param update_interval: How often to update the Q-network\n",
        "        (every update_interval steps)\n",
        "    :param learning_rate: The learning rate to use for the optimizer\n",
        "    :param batch_size: The minibatch size\n",
        "    :param gamma: The discount factor\n",
        "    :param n_hidden_units: Number of units for each hidden layer\n",
        "        of the Q-Network.\n",
        "    :param n_eval_episodes: The number of episodes to evaluate the policy on\n",
        "    :param evaluation_interval: How often to evaluate the policy\n",
        "    :param eval_exploration_rate: The exploration rate to use during evaluation\n",
        "    :param seed: Random seed for the pseudo random generator\n",
        "    :param eval_render_mode: The render mode to use for evaluation\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    # Seed Numpy as PyTorch pseudo random generators\n",
        "    # Seed Numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    th.manual_seed(seed)\n",
        "\n",
        "    # Create the environment\n",
        "    env = gym.make(env_id)\n",
        "    # For highway env\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    assert isinstance(env.action_space, spaces.Discrete)\n",
        "    env.action_space.seed(seed)\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(env_id, render_mode=eval_render_mode)\n",
        "    eval_env = gym.wrappers.FlattenObservation(eval_env)\n",
        "    eval_env.reset(seed=seed)\n",
        "    eval_env.action_space.seed(seed)\n",
        "\n",
        "    # Create the q-network\n",
        "    q_net = QNetwork(env.observation_space, env.action_space, n_hidden_units=n_hidden_units)\n",
        "    # Create the target network\n",
        "    q_target_net = QNetwork(env.observation_space, env.action_space, n_hidden_units=n_hidden_units)\n",
        "    # Copy the parameters of the q-network to the target network\n",
        "    q_target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "    # For flappy bird\n",
        "    if env.observation_space.dtype == np.float64:\n",
        "        q_net.double()\n",
        "        q_target_net.double()\n",
        "\n",
        "    # Create the optimizer, we only optimize the parameters of the q-network\n",
        "    optimizer = th.optim.Adam(q_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create the Replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_size, env.observation_space, env.action_space)\n",
        "    # Reset the env\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    for current_step in range(1, n_timesteps + 1):\n",
        "        # Update the current exploration schedule (update the value of epsilon)\n",
        "        exploration_rate = linear_schedule(\n",
        "            exploration_initial_eps,\n",
        "            exploration_final_eps,\n",
        "            current_step,\n",
        "            int(exploration_fraction * n_timesteps),\n",
        "        )\n",
        "        # Do one step in the environment following an epsilon-greedy policy\n",
        "        # and store the transition in the replay buffer\n",
        "        obs = collect_one_step(\n",
        "            env,\n",
        "            q_net,\n",
        "            replay_buffer,\n",
        "            obs,\n",
        "            exploration_rate=exploration_rate,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        # Update the target network\n",
        "        # by copying the parameters from the Q-network every target_network_update_interval steps\n",
        "        if (current_step % target_network_update_interval) == 0:\n",
        "            q_target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        # Update the Q-network every update_interval steps\n",
        "        # after learning_starts steps have passed (warmup phase)\n",
        "        if (current_step % update_interval) == 0 and current_step > learning_starts:\n",
        "            # Do one gradient step\n",
        "            dqn_update(q_net, q_target_net, optimizer, replay_buffer, batch_size, gamma=gamma)\n",
        "\n",
        "        if (current_step % evaluation_interval) == 0:\n",
        "            print()\n",
        "            print(f\"Evaluation at step {current_step}:\")\n",
        "            print(f\"exploration_rate={exploration_rate:.2f}\")\n",
        "            # Evaluate the current greedy policy (deterministic policy)\n",
        "            evaluate_policy(eval_env, q_net, n_eval_episodes, eval_exploration_rate=eval_exploration_rate)\n",
        "            # Save a checkpoint\n",
        "            th.save(q_net.state_dict(), f\"../logs/q_net_checkpoint_{env_id}_{current_step}.pth\")\n",
        "    return q_net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14af9427-8166-40fa-a581-51599d3be306",
      "metadata": {
        "id": "14af9427-8166-40fa-a581-51599d3be306"
      },
      "source": [
        "## Train DQN agent with target network on CartPole env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55922877-c7b1-4772-ad1a-e4c1f3ac67d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55922877-c7b1-4772-ad1a-e4c1f3ac67d6",
        "outputId": "01aa9c12-42a9-4ac6-f23f-d9764fb9fa90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation at step 5000:\n",
            "exploration_rate=0.40\n",
            "Mean episode reward: 248.30 +/- 108.67\n",
            "\n",
            "Evaluation at step 10000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 266.20 +/- 53.94\n",
            "\n",
            "Evaluation at step 15000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 165.20 +/- 28.99\n",
            "\n",
            "Evaluation at step 20000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 221.50 +/- 28.89\n",
            "\n",
            "Evaluation at step 25000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 276.70 +/- 58.35\n",
            "\n",
            "Evaluation at step 30000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 268.70 +/- 30.26\n",
            "\n",
            "Evaluation at step 35000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 9.90 +/- 0.70\n",
            "\n",
            "Evaluation at step 40000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 45000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 50000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 55000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 60000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 65000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 70000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 75000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 80000:\n",
            "exploration_rate=0.04\n",
            "Mean episode reward: 500.00 +/- 0.00\n"
          ]
        }
      ],
      "source": [
        "# Tuned hyperparameters from the RL Zoo3 of the Stable Baselines3 library\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "q_net = run_dqn(\n",
        "    env_id=env_id,\n",
        "    replay_buffer_size=100_000,\n",
        "    # Note: you can remove the target network\n",
        "    # by setting target_network_update_interval=1\n",
        "    target_network_update_interval=10,\n",
        "    learning_starts=1000,\n",
        "    exploration_initial_eps=1.0,\n",
        "    exploration_final_eps=0.04,\n",
        "    exploration_fraction=0.1,\n",
        "    n_timesteps=80_000,\n",
        "    update_interval=2,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    n_eval_episodes=10,\n",
        "    evaluation_interval=5000,\n",
        "    # No exploration during evaluation\n",
        "    # (deteministic policy)\n",
        "    eval_exploration_rate=0.0,\n",
        "    seed=2022,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52450a1f-60b4-4098-bc64-5860476a50cc",
      "metadata": {
        "id": "52450a1f-60b4-4098-bc64-5860476a50cc"
      },
      "source": [
        "### Visualize the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e18bf9-e3e4-495d-be4b-6acf0ae28ee1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "56e18bf9-e3e4-495d-be4b-6acf0ae28ee1",
        "outputId": "9b166dcb-7888-4d1f-8d00-e30f730556f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video to /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_CartPole-v1.mp4\n",
            "Moviepy - Building video /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_CartPole-v1.mp4.\n",
            "Moviepy - Writing video /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_CartPole-v1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_CartPole-v1.mp4\n",
            "Mean episode reward: 500.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.0\n",
        "video_name = f\"DQN_{env_id}\"\n",
        "\n",
        "# Optional: load checkpoint\n",
        "# q_net = QNetwork(eval_env.observation_space, eval_env.action_space, n_hidden_units=64)\n",
        "# q_net.load_state_dict(th.load(\"../logs/q_net_checkpoint_CartPole-v1_75000.pth\"))\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(\"../logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642e02d8-bd9c-4a07-9a0d-86748dcdd391",
      "metadata": {
        "id": "642e02d8-bd9c-4a07-9a0d-86748dcdd391"
      },
      "source": [
        "## Training DQN agent on flappy bird:\n",
        "\n",
        "You can go in the [GitHub repo](https://github.com/araffin/flappy-bird-gymnasium/tree/patch-1) to learn more about this environment.\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://raw.githubusercontent.com/markub3327/flappy-bird-gymnasium/main/imgs/dqn.gif\" width=\"300\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e9703d-7b5b-4188-9737-786476a627ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75e9703d-7b5b-4188-9737-786476a627ec",
        "outputId": "af33f674-5a4d-45ae-d947-ccfdd571fb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1\n",
            "  Cloning https://github.com/araffin/flappy-bird-gymnasium (to revision patch-1) to /tmp/pip-install-mp0ditl3/flappy-bird-gymnasium_f1fabc37a088495fafefa1b2d7c4ad4a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/araffin/flappy-bird-gymnasium /tmp/pip-install-mp0ditl3/flappy-bird-gymnasium_f1fabc37a088495fafefa1b2d7c4ad4a\n",
            "  Running command git checkout -b patch-1 --track origin/patch-1\n",
            "  Switched to a new branch 'patch-1'\n",
            "  Branch 'patch-1' set up to track remote branch 'patch-1' from 'origin'.\n",
            "  Resolved https://github.com/araffin/flappy-bird-gymnasium to commit 8828737242a38c0cd18c4260819fe7e695fb6bae\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (from flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1) (0.29.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1) (1.25.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1) (2.5.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium->flappy-bird-gymnasium@ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1) (0.0.4)\n",
            "Building wheels for collected packages: flappy-bird-gymnasium\n",
            "  Building wheel for flappy-bird-gymnasium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flappy-bird-gymnasium: filename=flappy_bird_gymnasium-0.2.2-py3-none-any.whl size=1074470 sha256=0cd689aa259b935d1e4ead172b6b6661fc1f1be167bafce97066f5b5c25193ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fodnwe1w/wheels/50/4a/c9/4cc2151ba0b70e402b5f62fce27724f287958b51df0f3b3938\n",
            "Successfully built flappy-bird-gymnasium\n",
            "Installing collected packages: flappy-bird-gymnasium\n",
            "Successfully installed flappy-bird-gymnasium-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install \"flappy-bird-gymnasium @ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a0eef2-d652-4224-86c6-8f669e82ec3e",
      "metadata": {
        "id": "e2a0eef2-d652-4224-86c6-8f669e82ec3e"
      },
      "outputs": [],
      "source": [
        "import flappy_bird_gymnasium  # noqa: F401"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf79677e-561c-4bb5-95d2-d4a99eb0579c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf79677e-561c-4bb5-95d2-d4a99eb0579c",
        "outputId": "aac5f172-8d1c-46b8-88db-3bfc20acbee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation at step 50000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 9.00 +/- 0.00\n",
            "\n",
            "Evaluation at step 100000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 10.96 +/- 2.74\n",
            "\n",
            "Evaluation at step 150000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 23.70 +/- 6.85\n",
            "\n",
            "Evaluation at step 200000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 20.00 +/- 2.45\n",
            "\n",
            "Evaluation at step 250000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 42.56 +/- 9.89\n",
            "\n",
            "Evaluation at step 300000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 94.28 +/- 96.05\n",
            "\n",
            "Evaluation at step 350000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 122.60 +/- 170.07\n",
            "\n",
            "Evaluation at step 400000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 222.18 +/- 109.94\n",
            "\n",
            "Evaluation at step 450000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 193.14 +/- 209.77\n",
            "\n",
            "Evaluation at step 500000:\n",
            "exploration_rate=0.03\n",
            "Mean episode reward: 78.02 +/- 44.26\n"
          ]
        }
      ],
      "source": [
        "env_id = \"FlappyBird-v0\"\n",
        "\n",
        "q_net = run_dqn(\n",
        "    env_id=env_id,\n",
        "    replay_buffer_size=100_000,\n",
        "    # Note: you can remove the target network\n",
        "    # by setting target_network_update_interval=1\n",
        "    target_network_update_interval=250,\n",
        "    learning_starts=10_000,\n",
        "    exploration_initial_eps=1.0,\n",
        "    exploration_final_eps=0.03,\n",
        "    exploration_fraction=0.1,\n",
        "    n_timesteps=500_000,\n",
        "    update_interval=4,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=128,\n",
        "    gamma=0.98,\n",
        "    n_eval_episodes=5,\n",
        "    evaluation_interval=50000,\n",
        "    n_hidden_units=256,\n",
        "    # No exploration during evaluation\n",
        "    # (deteministic policy)\n",
        "    eval_exploration_rate=0.0,\n",
        "    seed=2023,\n",
        "    eval_render_mode=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a08cc96-2601-49dc-921c-44e2a048df81",
      "metadata": {
        "id": "9a08cc96-2601-49dc-921c-44e2a048df81"
      },
      "source": [
        "### Record a video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e191f2-e3b3-41d0-bff2-50e1b9c9d19c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "e9e191f2-e3b3-41d0-bff2-50e1b9c9d19c",
        "outputId": "2fbc46f7-18ed-4d4f-c6f8-bcf5476a452b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video to /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_FlappyBird-v0.mp4\n",
            "Moviepy - Building video /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_FlappyBird-v0.mp4.\n",
            "Moviepy - Writing video /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_FlappyBird-v0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /usr/local/lib/python3.10/dist-packages/logs/videos/DQN_FlappyBird-v0.mp4\n",
            "Mean episode reward: 24.33 +/- 7.88\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.00\n",
        "video_name = f\"DQN_{env_id}\"\n",
        "\n",
        "\n",
        "# Optional: load checkpoint\n",
        "q_net = QNetwork(eval_env.observation_space, eval_env.action_space, n_hidden_units=256)\n",
        "# Convert weights from float32 to float64 to match flappy bird obs\n",
        "q_net.double()\n",
        "q_net.load_state_dict(th.load(\"../logs/q_net_checkpoint_FlappyBird-v0_200000.pth\"))\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(\"../logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a60230-6477-4318-8d60-10f6eada6064",
      "metadata": {
        "id": "69a60230-6477-4318-8d60-10f6eada6064"
      },
      "source": [
        "### Going further\n",
        "\n",
        "- analyse the learned q-values\n",
        "- explore different value for the target update, use soft update instead of hard-copy\n",
        "- experiment with Huber loss (smooth l1 loss) instead of l2 loss (mean squared error)\n",
        "- play with different environments\n",
        "- implement a CNN to play flappybird/pong from pixels (need to stack frames)\n",
        "- implement DQN extensions (double Q-learning, prioritized experience replay, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be2cc11-79e6-40c6-ab61-f78fec498315",
      "metadata": {
        "id": "6be2cc11-79e6-40c6-ab61-f78fec498315"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you have seen how to implement the DQN algorithm (update rule and training loop) using all the components from part I (replay buffer, epsilon-greedy exploration strategy, Q-Network, ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c3f25b-4eed-456a-b4f7-68ce16f56763",
      "metadata": {
        "id": "89c3f25b-4eed-456a-b4f7-68ce16f56763"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}